# -*- coding: utf-8 -*-
"""Kmeans_Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UUiLpawRmoucHza2580CsXgpFQ7f2SWX
"""

!pip install kneed
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from kneed import KneeLocator
from sklearn.metrics import silhouette_score # Import the missing function
from sklearn.decomposition import PCA
from matplotlib.colors import LinearSegmentedColormap # Import the required class
import seaborn as sns

# Sample dataset
df = pd.read_csv('/content/cmci_total_latlong.csv')

# Encode 'Status' column
df['Status'] = df['Status'].map({'Tapped': 1, 'Untapped': 0})

df.columns

df.info()

df.head()

df.isnull().sum()

numerical_columns = df.select_dtypes(include=['number'])

numerical_columns.corr()

df1 = df.copy()

for col in numerical_columns:
    if col in df1.columns: # Check if the column exists in df1
        plt.figure(figsize=(4, 3))
        sns.boxplot(x=col, data=df1)
        plt.title(f'Box Plot of {col}')
        plt.show()

# Use IQR method to remove outliers

for col in numerical_columns:
    if col in df1.columns:  # Check if column exists before processing
        q1 = df1[col].quantile(0.25)
        q3 = df1[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - (1.5 * iqr)
        upper_bound = q3 + (1.5 * iqr)
        df2 = df1[(df1[col] >= lower_bound) & (df1[col] <= upper_bound)]  # Update df2 based on outliers in current 'col'

        plt.figure(figsize=(4, 3))
        sns.boxplot(x=col, data=df2)
        plt.title(f'Box Plot of {col}')
        plt.show()

df2.shape

df2.columns

# Create a heatmap of the correlation matrix
# Compute the correlation matrix
correlation_matrix = numerical_columns.corr()
plt.figure(figsize=(10, 8))  # Adjust the size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)

# Set the title and labels
plt.title('Correlation Matrix Heatmap')
plt.show()

df3 = df2.copy(deep=True)

df3.drop(columns=['Utilities', 'Latitude', 'Longitude'], inplace = True)

df3

"""**STAAAART**"""

# Select features for clustering
features = df3[['Local Economy Size',
       'Employment Generation', 'Cost of Living',
       'Capacity of Health Services', 'Capacity of School Services',
       'Peace and Order', 'Social Protection',
       'Availability of Basic Utilities', 'Transportation Vehicles',
       'Education', 'Health', 'Employed Population',
       'Internet Capability', 'Poverty Incidence']]

from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

# Reduce dimensions of data
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

model = KMeans(n_clusters=5)
model.fit(X_pca)
sil_score = silhouette_score(X_pca, model.labels_)
print("Silhouette Score:", sil_score)

X_scaled

X_pca

import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

# Initialize a list to store silhouette scores
sil_scores = []

# Loop over a range of cluster numbers to compute silhouette scores
k = range(2, 11)
for i in k:
    model = AgglomerativeClustering(n_clusters=i)
    model.fit(X_pca)
    sil_scores.append(silhouette_score(X_pca, model.labels_))

# Find the maximum silhouette score and its corresponding number of clusters
max_silhouette_score = max(sil_scores)
best_cluster_num = sil_scores.index(max_silhouette_score) + 2  # +2 because range starts from 2

# Print the best number of clusters
print(f"The best number of clusters based on the silhouette score is: {best_cluster_num}")

# Plot the silhouette scores to visualize the results
plt.plot(range(2, 11), sil_scores, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Scores for Different Number of Clusters')
plt.show()

# Use elbow method to check best optimal cluster
sil_scores = []
for i in range(2, 11):
    model = AgglomerativeClustering(n_clusters=i)
    model.fit(X_pca)
    sil_scores.append(silhouette_score(X_pca, model.labels_))

plt.plot(range(2, 11), sil_scores)
plt.show()

model.labels_

n = 3
model = KMeans(n_clusters=n)
model.fit(X_pca)

cluster_labels = model.fit_predict(X_pca)
sil_score = silhouette_score(X_pca, model.labels_)
print("Silhouette Score:", sil_score)

# Define custom colors
custom_palette = {
    0: '#328740',  # Chill Green
    1: '#f08d0c',  # Orange
    2: '#0000FF'   # Blue
}

sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=model.labels_)
plt.title(f"KMeans clustering with {n} clusters")
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

model = AgglomerativeClustering(n_clusters=n)
model.fit(X_pca)
sil_score = silhouette_score(X_pca, model.labels_)
print("Silhouette Score:", sil_score)

sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=model.labels_)
plt.title(f"Agglomerative clustering with {n} clusters")
plt.show()

"""**INTERPRET**"""

df3['cluster_labels'] = cluster_labels

df3.columns

from pandas.plotting import parallel_coordinates

# Assuming 'cluster_labels' is a variable containing the cluster assignments
# from your clustering algorithm
df3['Cluster'] = cluster_labels  # Assign the cluster labels to a new column in df3

# centroids for the numeric features only
numeric_features = df3.select_dtypes(include=[np.number]).columns
numeric_centroids = df3[numeric_features].groupby('Cluster').mean().reset_index()

# Parallel Coordinates Plot for the Centroids
plt.figure(figsize=(14, 7))
# Assuming 'parallel_coordinates' is a function you've defined or imported
parallel_coordinates(numeric_centroids, 'Cluster', color=plt.cm.tab10.colors)
plt.xticks(rotation=90)
plt.title('Parallel Coordinates Plot of Clusters')
plt.show()

numeric_centroids

numeric_features

df3.columns

### create parallel coordinates per pillar
economic_dynamism = df3[['Local Economy Size','Employment Generation', 'Cost of Living']] # Use a list of column names
governmnt_efficiency = df3[['Capacity of Health Services', 'Capacity of School Services','Peace and Order', 'Social Protection']]
infra = df3[['Availability of Basic Utilities', 'Transportation Vehicles',
       'Education', 'Health']]
resiliency = df3[['Employed Population']] # Use a list even for a single column
innovation = df3 [['Internet Capability','Poverty Incidence']]
poverty = df3 [['Poverty Incidence']]

# Compute centroids for the Economic Dynamism features
df3['Cluster'] = cluster_labels  # Assign the cluster labels to a new column in df3
numeric_features = innovation.select_dtypes(include=[np.number])
numeric_centroids = numeric_features.groupby(df3['Cluster']).mean().reset_index()

# Plot Parallel Coordinates for Economic Dynamism
plt.figure(figsize=(14, 7))
parallel_coordinates(numeric_centroids, 'Cluster', color=plt.cm.tab10.colors)
plt.xticks(rotation=90)
plt.title('Parallel Coordinates Plot for Economic Dynamism')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

# Compute centroids for the Economic Dynamism features
df3['Cluster'] = cluster_labels  # Assign the cluster labels to a new column in df3
numeric_features = economic_dynamism.select_dtypes(include=[np.number])
numeric_centroids = numeric_features.groupby(df3['Cluster']).mean().reset_index()

# Plot Parallel Coordinates for Economic Dynamism
plt.figure(figsize=(14, 7))
parallel_coordinates(numeric_centroids, 'Cluster', color=plt.cm.tab10.colors)
plt.xticks(rotation=90)
plt.title('Parallel Coordinates Plot for Economic Dynamism')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

# Compute centroids for the Economic Dynamism features
df3['Cluster'] = cluster_labels  # Assign the cluster labels to a new column in df3
numeric_features = governmnt_efficiency.select_dtypes(include=[np.number])
numeric_centroids = numeric_features.groupby(df3['Cluster']).mean().reset_index()

# Plot Parallel Coordinates for Economic Dynamism
plt.figure(figsize=(14, 7))
parallel_coordinates(numeric_centroids, 'Cluster', color=plt.cm.tab10.colors)
plt.xticks(rotation=90)
plt.title('Parallel Coordinates Plot for Economic Dynamism')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

# Compute centroids for the Economic Dynamism features
df3['Cluster'] = cluster_labels  # Assign the cluster labels to a new column in df3
numeric_features = infra.select_dtypes(include=[np.number])
numeric_centroids = numeric_features.groupby(df3['Cluster']).mean().reset_index()

# Plot Parallel Coordinates for Economic Dynamism
plt.figure(figsize=(14, 7))
parallel_coordinates(numeric_centroids, 'Cluster', color=plt.cm.tab10.colors)
plt.xticks(rotation=90)
plt.title('Parallel Coordinates Plot for Economic Dynamism')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

# Compute centroids for the Economic Dynamism features
df3['Cluster'] = cluster_labels  # Assign the cluster labels to a new column in df3
numeric_features = resiliency.select_dtypes(include=[np.number])
numeric_centroids = numeric_features.groupby(df3['Cluster']).mean().reset_index()

# Plot Parallel Coordinates for Economic Dynamism
plt.figure(figsize=(14, 7))
parallel_coordinates(numeric_centroids, 'Cluster', color=plt.cm.tab10.colors)
plt.xticks(rotation=90)
plt.title('Parallel Coordinates Plot for Economic Dynamism')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

# Compute centroids for the Economic Dynamism features
df3['Cluster'] = cluster_labels  # Assign the cluster labels to a new column in df3
numeric_features = innovation.select_dtypes(include=[np.number])
numeric_centroids = numeric_features.groupby(df3['Cluster']).mean().reset_index()

# Plot Parallel Coordinates for Economic Dynamism
plt.figure(figsize=(14, 7))
parallel_coordinates(numeric_centroids, 'Cluster', color=plt.cm.tab10.colors)
plt.xticks(rotation=90)
plt.title('Parallel Coordinates Plot for Economic Dynamism')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

numeric_centroids

# Initialize the KMeans model
k = 3
kmeans = KMeans(n_clusters=k)

# Fit the model to your data
kmeans.fit(features)

# Now you can access the centroids
centroids = kmeans.cluster_centers_

# Create a DataFrame for the centroids
centroid_df = pd.DataFrame(centroids, columns=features.columns)
centroid_df['Cluster'] = range(k)

centroid_df

### EXPORT THE DATA ###

from google.colab import files
centroid_df.to_csv('centroids.csv', index=False)
# Download the file
files.download('centroids.csv')

# Calculate the distances of each toy to each cluster centroid
distances = kmeans.transform(X_scaled)

# Create a DataFrame for the distances
distance_df = pd.DataFrame(distances, columns=[f'Distance_to_Cluster_{i}' for i in range(k)])

# Merge the distance DataFrame with the original data
result_df = pd.concat([df3, distance_df], axis=1)

result_df

# Calculate feature importance for each point
def compute_feature_importance(X_scaled, centroids, labels):
    feature_importance = []
    for i in range(len(X_scaled)):
        cluster = labels[i]
        centroid = centroids[cluster]
        importance = np.abs(X_scaled[i] - centroid)
        feature_importance.append(importance)

    return np.array(feature_importance)

# Compute feature importance
feature_importance = compute_feature_importance(X_scaled, centroids, cluster_labels)

# Get the column names from the original DataFrame (df3) before it was scaled
features = df3.columns[2:16]
feature_importance_df = pd.DataFrame(feature_importance, columns=features)

# Average feature importance across all points in each cluster
avg_feature_importance = feature_importance_df.groupby(df3['Cluster']).mean()

# Visualize feature importance for each cluster separately (horizontal, sorted)
for cluster in avg_feature_importance.index:
    plt.figure(figsize=(10, 5))
    # Sort feature importances for the current cluster in descending order
    sorted_importances = avg_feature_importance.loc[cluster].sort_values(ascending=True)
    sorted_importances.plot(kind='barh')  # Use 'barh' for horizontal bar chart
    plt.title(f'Feature Importance for Cluster {cluster}')
    plt.xlabel('Average Importance')
    plt.ylabel('Features')
    plt.show()

### EXPORT THE DATA ###

from google.colab import files
result_df.to_csv('distance.csv', index=False)
# Download the file
files.download('distance.csv')

### EXPORT THE DATA ###

from google.colab import files
df3.to_csv('clusteredData.csv', index=False)
# Download the file
files.download('clusteredData.csv')